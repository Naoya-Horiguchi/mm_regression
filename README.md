# MM regression kernel test tool

## Prerequisite

- kernel source is available on /src/linux-dev
- some packages `gcc`, `ruby`, and `numactl` must be available on your test system.

## Note

- This test tool has many page migration testcases that are meaningful only
  on NUMA systems, so please run this test tool on a NUMA system.

## Recipe list

- This testsuite is recipe based, which means that each testcase is defined
  by an associated recipe file. All recipe files are available under `cases/`.
- Some testcases are similar and only differs in a few parameters. So they might
  be defined in recipe set file (files with extension `.set` under cases/.)
  A recipe set file is used to derive (with command `make split_recipes` or
  `make update_recipes`) recipe files for dependent testcases,
  which likely to have extension `.auto`.
- You can see the list of (runnable) recipes are generated by command `make allrecipes`.

## Run test

The simplest way to run this test suite is to run `run.sh`.
This script accepts some parameters from callers.
For example, if you try to run one specific testcase `cases/foo`,
ou can do this with the following command:

    RECIPEFILES=cases/foo ./run.sh

You can give multiple recipes like:

    RECIPEFILES="cases/foo cases/bar" ./run.sh

`RECIPEFILES` is handled as a file pathname, so you can use wildcard `*`:

    RECIPEFILES="cases/foo*" ./run.sh

Combining with `make allrecipes`, you can run some group of testcases:

    RECIPEFILES="$(make allrecipes | grep foo)" ./run.sh

There're some other useful options/environment variables:
- `RUNNAME`: you can give ID for each run, which is mainly useful to separate
  result directory. Test results are store under `work/$RUNNAME`. If `RUNNAME`
  is not given, timestamp (`date +%y%m%d_%H%M%S` form) is used.
- `AGAIN`: in order to make automation easier, if you run the test with `RUNNAME`
  with which you already run other testcases, "already finished" testcases
  are skipped. To cancel this behavior and overwrite existing result, give
  some non-empty string to this variable. Note that existing test results
  are removed.
- `SOFT_RETRY`: some testcase has a characteristic that the test occationally
  fails for a good reason (for example page migration.) IOW, one failure
  doesn't mean the teescase's failure. If you set `SOFT_RETRY`, one success
  in `SOFT_RETRY` trials is considered as testcase success.
- `HARD_RETRY`: contrary to `SOFT_RETRY`, if some testcase needs to succeed
  multiple times to be considered as testcase success, you can set this
  variable.
- `HIGHEST_PRIORITY` (Default `0`): some testcases are not appropriate to be
  run in your regular testing for various reasons (the testcase under
  developing, obsolete, and/or known to cause kernel panic ...).
  If you set `PRORITY=` in your testcase recipe, and run with
  `HIGHEST_PRIORITY` (`>= PRIORITY`), the testcases is skipped in the current
  run. In priority value, lower value is highest priority (`0` is highest.)
  Default value of `PRIORITY` (given to every testcase) is `10`.
- `LOWEST_PRIORITY` (Default `10`): similar to `HIGHEST_PRIORITY`. If you want
  to run the testcase with lower priority (higher `PRIORITY` value,) give
  some larger value to `LOWEST_PRIORITY`.

## Result

- Test result are stored under `work/$RUNNAME`. The results of each testcase
  is saved under `work/$RUNNAME/$RECIPEID`.
- A test summary script is available:

    [build1:~/upstream/mm_regression]$ make summary
    PASS 149, FAIL 12, WARN 1, SKIP 23, NONE 249

## How to define new testcase?

## Contact
- Naoya Horiguchi <n-horiguchi@ah.jp.nec.com> / <nao.horiguchi@gmail.com>
